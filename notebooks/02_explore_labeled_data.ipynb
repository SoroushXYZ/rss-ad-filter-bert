{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explore Labeled RSS Data\n",
        "\n",
        "This notebook loads the collected RSS articles and their labels to explore the dataset before model training.\n",
        "\n",
        "## Goals:\n",
        "1. Load RSS articles and labels\n",
        "2. Explore the dataset structure\n",
        "3. Show examples of each class (advertisement vs news)\n",
        "4. Analyze label distribution\n",
        "5. Prepare data for future model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import glob\n",
        "import os\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading articles from: ../data/rss_articles_20250922_000542.json\n",
            "âœ… Loaded 1608 articles\n",
            "Loading labels from: ../data/labels_20250922_112846.json\n",
            "âœ… Loaded 1213 labels\n",
            "\n",
            "DataFrame shape: (1608, 8)\n",
            "Columns: ['source', 'title', 'link', 'description', 'published', 'summary', 'tags', 'feed_url']\n"
          ]
        }
      ],
      "source": [
        "# Load the latest RSS articles and labels\n",
        "data_dir = '../data'\n",
        "\n",
        "# Load articles\n",
        "articles_files = glob.glob(os.path.join(data_dir, 'rss_articles_*.json'))\n",
        "latest_articles_file = max(articles_files, key=os.path.getctime)\n",
        "print(f\"Loading articles from: {latest_articles_file}\")\n",
        "\n",
        "with open(latest_articles_file, 'r', encoding='utf-8') as f:\n",
        "    articles = json.load(f)\n",
        "\n",
        "print(f\"âœ… Loaded {len(articles)} articles\")\n",
        "\n",
        "# Load labels\n",
        "labels_files = glob.glob(os.path.join(data_dir, 'labels_*.json'))\n",
        "latest_labels_file = max(labels_files, key=os.path.getctime)\n",
        "print(f\"Loading labels from: {latest_labels_file}\")\n",
        "\n",
        "with open(latest_labels_file, 'r', encoding='utf-8') as f:\n",
        "    labels_data = json.load(f)\n",
        "\n",
        "labels = labels_data['labels']\n",
        "print(f\"âœ… Loaded {len(labels)} labels\")\n",
        "\n",
        "# Convert to DataFrame for easier analysis\n",
        "df = pd.DataFrame(articles)\n",
        "print(f\"\\nDataFrame shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labeled articles: 1213\n",
            "\n",
            "Label distribution:\n",
            "label\n",
            "news             1034\n",
            "advertisement     179\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Label percentages:\n",
            "label\n",
            "news             85.2\n",
            "advertisement    14.8\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Add labels to DataFrame\n",
        "df['label'] = df.index.astype(str).map(labels)\n",
        "df['is_labeled'] = df['label'].notna()\n",
        "\n",
        "# Filter to only labeled articles\n",
        "labeled_df = df[df['is_labeled']].copy()\n",
        "print(f\"Labeled articles: {len(labeled_df)}\")\n",
        "\n",
        "# Label distribution\n",
        "label_counts = labeled_df['label'].value_counts()\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(label_counts)\n",
        "print(f\"\\nLabel percentages:\")\n",
        "print((label_counts / len(labeled_df) * 100).round(1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ“‹ ADVERTISEMENT EXAMPLES (3 shown)\n",
            "============================================================\n",
            "\n",
            "ðŸ”¹ Example 7:\n",
            "   Source: TechCrunch\n",
            "   Title: 6 days left: Last chance for Regular Bird savings for TechCrunch Disrupt 2025 passes\n",
            "   Description: Timeâ€™s ticking! Register by September 26 at 11:59 p.m. PT to lock in Regular-Bird pricing and save up to $668 on your pass to TechCrunch Disrupt 2025, happening in San Francisco's Moscone West on Octo...\n",
            "   Published: Sun, 21 Sep 2025 14:00:00 +0000\n",
            "--------------------------------------------------\n",
            "\n",
            "ðŸ”¹ Example 12:\n",
            "   Source: TechCrunch\n",
            "   Title: Only 7 days left to save on TechCrunch Disrupt 2025 tickets â€” lock in Regular Bird pricing now\n",
            "   Description: Time is running out to grab your pass to TechCrunch Disrupt 2025, happening October 27â€“29 in San Francisco. With less than 7 days left to lock in Regular Bird pricing, nowâ€™s your chance to save up to ...\n",
            "   Published: Sat, 20 Sep 2025 14:00:00 +0000\n",
            "--------------------------------------------------\n",
            "\n",
            "ðŸ”¹ Example 17:\n",
            "   Source: TechCrunch\n",
            "   Title: Best Apple Watch apps for boosting your productivity\n",
            "   Description: Although the Apple Watch comes with simple built-in productivity apps like Reminders and Calendar, itâ€™s worth exploring some third-party apps that are designed to boost productivity by offering additi...\n",
            "   Published: Fri, 19 Sep 2025 19:22:31 +0000\n",
            "--------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "ðŸ“‹ NEWS EXAMPLES (3 shown)\n",
            "============================================================\n",
            "\n",
            "ðŸ”¹ Example 1:\n",
            "   Source: TechCrunch\n",
            "   Title: VCs are still hiring MBAs, but firms are starting to need other experience more\n",
            "   Description: The MBA-to-VC pipeline remains a very real thing. But that path is a little shakier than it once was, according to PitchBook reporting and new academic research.\n",
            "   Published: Sun, 21 Sep 2025 22:23:40 +0000\n",
            "--------------------------------------------------\n",
            "\n",
            "ðŸ”¹ Example 2:\n",
            "   Source: TechCrunch\n",
            "   Title: Trump says Lachlan and Rupert Murdoch might invest in TikTok deal\n",
            "   Description: The Trump administration has been talking up a potential TikTok deal this weekend, with President Donald Trump telling Fox News on Sunday that Rupert Murdoch and his son Lachlan are \"probably\" going t...\n",
            "   Published: Sun, 21 Sep 2025 19:43:34 +0000\n",
            "--------------------------------------------------\n",
            "\n",
            "ðŸ”¹ Example 3:\n",
            "   Source: TechCrunch\n",
            "   Title: Silicon Valley bets big on â€˜environmentsâ€™ to train AI agents\n",
            "   Description: A wave of startups are creating RL environments to help AI labs train agents. It might be Silicon Valleyâ€™s next craze in the making.\n",
            "   Published: Sun, 21 Sep 2025 19:22:56 +0000\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Show examples of each class\n",
        "def show_examples(df, label, num_examples=5):\n",
        "    \"\"\"Show examples of a specific label\"\"\"\n",
        "    examples = df[df['label'] == label].head(num_examples)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ðŸ“‹ {label.upper()} EXAMPLES ({len(examples)} shown)\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for idx, row in examples.iterrows():\n",
        "        print(f\"\\nðŸ”¹ Example {idx + 1}:\")\n",
        "        print(f\"   Source: {row['source']}\")\n",
        "        print(f\"   Title: {row['title']}\")\n",
        "        if pd.notna(row['description']) and row['description']:\n",
        "            desc = row['description'][:200] + \"...\" if len(row['description']) > 200 else row['description']\n",
        "            print(f\"   Description: {desc}\")\n",
        "        print(f\"   Published: {row['published']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Show examples of each class\n",
        "for label in ['advertisement', 'news']:\n",
        "    if label in labeled_df['label'].values:\n",
        "        show_examples(labeled_df, label, num_examples=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ“Š LABEL DISTRIBUTION BY SOURCE\n",
            "============================================================\n",
            "label                      advertisement  news  total  ad_percentage  \\\n",
            "source                                                                 \n",
            "9to5Mac                                0    50     50            0.0   \n",
            "BBC News - Technology                  0    50     50            0.0   \n",
            "ExtremeTech                            6    44     50           12.0   \n",
            "Engadget                              13    37     50           26.0   \n",
            "DeepMind Blog                          1    49     50            2.0   \n",
            "Lifehacker                            26    24     50           52.0   \n",
            "Mashable (Tech)                       23    27     50           46.0   \n",
            "Google AI Blog (Research)              0    50     50            0.0   \n",
            "OpenAI Blog                            0    50     50            0.0   \n",
            "Tom's Hardware                         9    41     50           18.0   \n",
            "\n",
            "label                      news_percentage  \n",
            "source                                      \n",
            "9to5Mac                              100.0  \n",
            "BBC News - Technology                100.0  \n",
            "ExtremeTech                           88.0  \n",
            "Engadget                              74.0  \n",
            "DeepMind Blog                         98.0  \n",
            "Lifehacker                            48.0  \n",
            "Mashable (Tech)                       54.0  \n",
            "Google AI Blog (Research)            100.0  \n",
            "OpenAI Blog                          100.0  \n",
            "Tom's Hardware                        82.0  \n"
          ]
        }
      ],
      "source": [
        "# Analyze by source\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ðŸ“Š LABEL DISTRIBUTION BY SOURCE\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "source_analysis = labeled_df.groupby(['source', 'label']).size().unstack(fill_value=0)\n",
        "source_analysis['total'] = source_analysis.sum(axis=1)\n",
        "source_analysis['ad_percentage'] = (source_analysis['advertisement'] / source_analysis['total'] * 100).round(1)\n",
        "source_analysis['news_percentage'] = (source_analysis['news'] / source_analysis['total'] * 100).round(1)\n",
        "\n",
        "print(source_analysis.sort_values('total', ascending=False).head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ“ˆ DATASET SUMMARY FOR MODEL TRAINING\n",
            "============================================================\n",
            "Total articles collected: 1608\n",
            "Total articles labeled: 1213\n",
            "Labeling completion: 75.4%\n",
            "\n",
            "Class distribution:\n",
            "  news: 1034 articles (85.2%)\n",
            "  advertisement: 179 articles (14.8%)\n",
            "\n",
            "Sources represented: 53\n",
            "Unique sources: ['9to5Mac', 'AWS News Blog', 'All Things Distributed (AWS CTO)', 'Analytics Vidhya', 'Android Authority', 'Apple Newsroom', 'Ars Technica', 'BAIR (Berkeley AI Research) Blog', 'BBC News - Technology', 'Business Insider (Tech)', 'CNET News', 'CNN Technology', 'CloudTech News', 'CloudTweaks', 'Daring Fireball', 'Datanami', 'DeepMind Blog', 'Engadget', 'ExtremeTech', 'Facebook Newsroom (Meta)', 'GeekWire', 'Gizmodo', 'Google (The Keyword)', 'Google AI Blog (Research)', 'Google Cloud Blog', 'Hacker News (Top)', 'HuffPost Tech', 'KDnuggets', 'Kaggle Blog', 'Lifehacker', 'MIT Technology Review', 'Machine Learning Mastery', 'MarkTechPost (AI News)', 'Mashable (Tech)', 'Meta Engineering Blog', 'Meta Research Blog', 'Microsoft Azure Blog', 'NVIDIA Blog', 'NY Times - Technology', 'Official Microsoft Blog', 'OpenAI Blog', 'Recode (Vox Technology)', 'Slashdot', 'Stratechery (Ben Thompson)', 'TechCrunch', 'TechMeme', 'TechRepublic (Cloud)', 'The Guardian - Technology', 'The Verge', \"Tom's Hardware\", 'Towards Data Science', 'WIRED', 'ZDNet (All Topics)']\n",
            "\n",
            "Articles with missing descriptions: 0\n",
            "\n",
            "Text length statistics:\n",
            "Title length - Mean: 68, Max: 204\n",
            "Description length - Mean: 1232, Max: 64821\n"
          ]
        }
      ],
      "source": [
        "# Dataset summary for model training preparation\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ðŸ“ˆ DATASET SUMMARY FOR MODEL TRAINING\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "print(f\"Total articles collected: {len(df)}\")\n",
        "print(f\"Total articles labeled: {len(labeled_df)}\")\n",
        "print(f\"Labeling completion: {(len(labeled_df) / len(df) * 100):.1f}%\")\n",
        "\n",
        "print(f\"\\nClass distribution:\")\n",
        "for label, count in label_counts.items():\n",
        "    percentage = (count / len(labeled_df) * 100)\n",
        "    print(f\"  {label}: {count} articles ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nSources represented: {labeled_df['source'].nunique()}\")\n",
        "print(f\"Unique sources: {sorted(labeled_df['source'].unique())}\")\n",
        "\n",
        "# Check for missing descriptions\n",
        "missing_desc = labeled_df['description'].isna().sum()\n",
        "print(f\"\\nArticles with missing descriptions: {missing_desc}\")\n",
        "\n",
        "# Text length analysis\n",
        "labeled_df['title_length'] = labeled_df['title'].str.len()\n",
        "labeled_df['desc_length'] = labeled_df['description'].str.len()\n",
        "\n",
        "print(f\"\\nText length statistics:\")\n",
        "print(f\"Title length - Mean: {labeled_df['title_length'].mean():.0f}, Max: {labeled_df['title_length'].max()}\")\n",
        "print(f\"Description length - Mean: {labeled_df['desc_length'].mean():.0f}, Max: {labeled_df['desc_length'].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ’¾ SAVING PROCESSED DATASET\n",
            "============================================================\n",
            "Training dataset shape: (1213, 5)\n",
            "Columns: ['title', 'description', 'source', 'label', 'text']\n",
            "âœ… Saved training dataset to: ../data/training_dataset.csv\n",
            "âœ… Saved training dataset to: ../data/training_dataset.json\n",
            "\n",
            "ðŸŽ¯ Ready for model training with 1213 labeled examples!\n"
          ]
        }
      ],
      "source": [
        "# Save processed dataset for future model training\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"ðŸ’¾ SAVING PROCESSED DATASET\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Create training-ready dataset\n",
        "training_data = labeled_df[['title', 'description', 'source', 'label']].copy()\n",
        "\n",
        "# Combine title and description for training\n",
        "training_data['text'] = training_data['title'] + ' ' + training_data['description'].fillna('')\n",
        "\n",
        "# Remove any rows with empty text\n",
        "training_data = training_data[training_data['text'].str.strip() != '']\n",
        "\n",
        "print(f\"Training dataset shape: {training_data.shape}\")\n",
        "print(f\"Columns: {training_data.columns.tolist()}\")\n",
        "\n",
        "# Save to CSV for easy loading later\n",
        "output_file = '../data/training_dataset.csv'\n",
        "training_data.to_csv(output_file, index=False)\n",
        "print(f\"âœ… Saved training dataset to: {output_file}\")\n",
        "\n",
        "# Also save as JSON for alternative loading\n",
        "output_json = '../data/training_dataset.json'\n",
        "training_data.to_json(output_json, orient='records', indent=2)\n",
        "print(f\"âœ… Saved training dataset to: {output_json}\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ Ready for model training with {len(training_data)} labeled examples!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "This notebook has:\n",
        "1. âœ… Loaded your labeled RSS data\n",
        "2. âœ… Analyzed label distribution \n",
        "3. âœ… Shown examples of each class\n",
        "4. âœ… Analyzed sources and text characteristics\n",
        "5. âœ… Created training-ready dataset files\n",
        "\n",
        "**Files created:**\n",
        "- `../data/training_dataset.csv` - CSV format for easy loading\n",
        "- `../data/training_dataset.json` - JSON format for alternative loading\n",
        "\n",
        "**Ready for future model training notebook!** ðŸš€\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
